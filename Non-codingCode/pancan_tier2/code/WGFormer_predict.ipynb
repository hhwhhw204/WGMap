{"cells":[{"cell_type":"code","source":["!pip install shap==0.37.0"],"metadata":{"id":"Ums5kb0mih5l"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F-43Va5j-o_K"},"outputs":[],"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","import argparse\n","import sys\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import Adam\n","from math import sqrt\n","import numpy as np\n","import random\n","import pandas as pd\n","import re\n","import matplotlib.pyplot as plt\n","from torch.utils.data import DataLoader\n","from sklearn.metrics import roc_curve\n","from sklearn.preprocessing import MinMaxScaler,StandardScaler,RobustScaler,MaxAbsScaler  # 标准化工具\n","from sklearn import preprocessing\n","from sklearn.metrics import precision_recall_curve,roc_curve,average_precision_score\n","import warnings\n","import torch\n","from sklearn.metrics import auc\n","import os\n","import math\n","from tensorflow.keras.utils import get_custom_objects\n","from scipy.stats import fisher_exact\n","from scipy import stats\n","import joblib\n","import shap\n","\n","warnings.filterwarnings('ignore')\n","seed = 1\n","random.seed(seed)\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","tf.random.set_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","#找roc的最优阈值\n","def Find_Optimal_Cutoff(TPR, FPR, threshold):\n","    y = TPR - FPR\n","    Youden_index = np.argmax(y)  # Only the first occurrence is returned.\n","    optimal_threshold = threshold[Youden_index]\n","    point = [FPR[Youden_index], TPR[Youden_index]]\n","    return optimal_threshold, point\n","\n","def fisher_ex(a, b, c, d):\n","    _, pvalue = fisher_exact([[a, b], [c, d]], 'greater')\n","    if pvalue < 1e-256:\n","        pvalue = 1e-256\n","    #p1 = -math.log10(pvalue)\n","    return pvalue\n","\n","def build_set(all_list):\n","  pos_ids_noncds = []\n","  rna_ids = []\n","  neg_ids_noncds = []\n","  test_ids_ex = []\n","  tjumirna_list = []\n","\n","  df_pos_noncds = pd.read_csv(r'/content/drive/MyDrive/fulingya/Target/data/noncds_2.2.csv',sep=',')\n","  pos_ids_noncds2 = df_pos_noncds['geneid'].values.tolist()\n","  df_3 = pd.read_csv(r'/content/drive/MyDrive/fulingya/Target/data/noncds_3_new.csv',sep=',')\n","  pos_ids_noncds3 = df_3['geneid'].values.tolist()\n","  for i in pos_ids_noncds3:\n","    if i not in pos_ids_noncds2:\n","      pos_ids_noncds2.append(i)\n","  pos_ids = list(set(pos_ids_noncds2))\n","  #随机选择负样本，只能选非编码区\n","  excludes_ids = pos_ids\n","  df_all_list = pd.DataFrame(all_list)\n","  df_all_list.columns = ['id']\n","  df_ffu = df_all_list[~df_all_list['id'].isin(excludes_ids)]\n","  all_list_ffu = df_ffu['id'].values.tolist()\n","  for id in all_list_ffu:\n","    tmps_ffu = re.split('::', id)\n","    reg_ffu = tmps_ffu[0]\n","    reg_ffu1 = tmps_ffu[1]\n","    if 'tjumirna' in reg_ffu1:\n","      tjumirna_list.append(id)\n","    if 'cds' not in reg_ffu and 'tjumirna' not in reg_ffu1:\n","      neg_ids_noncds.append(id) #非编码区其他负样本基因\n","  df_neg_ids_noncds = pd.DataFrame(neg_ids_noncds)\n","  df_fu = df_neg_ids_noncds.sample(n=len(pos_ids)*10,random_state = 0,replace = False)\n","  neg_ids = df_fu[0].values.tolist()\n","\n","  for id in all_list:\n","    tmps = re.split('::', id)\n","    gene = tmps[2]\n","    reg = tmps[0]\n","    reg1 = tmps[1]\n","    if 'cds' in reg:\n","      test_ids_ex.append(id)\n","  test_ids = list(set(all_list)-set(test_ids_ex)-set(tjumirna_list))\n","  #test_ids = list(set(all_list)-set(test_ids_ex))\n","  pos_ids.sort()\n","  neg_ids.sort()\n","  test_ids.sort()\n","  print(len(pos_ids))\n","  print(len(neg_ids))\n","  #print(len(tjumirna_list))\n","  print(len(test_ids))\n","  return pos_ids, neg_ids, test_ids\n","\n","def file2data(cancer_type, train_pos, train_neg, test_ids):\n","    mode_all = ['all', 'mut2', 'cadd','css']\n","    tumors_file = '/content/drive/MyDrive/fulingya/MDriver/tumors.txt'\n","    tumors_set = {'Pancan': 'Pancan'}\n","    for line in open(tumors_file, 'rt'):\n","        txt = line.rstrip().split('\\t')\n","        tumors_set[txt[0]] = txt[1]\n","    X_train = []\n","    X_test = []\n","    X = []\n","    for mode in mode_all:\n","      if mode != 'mut2':\n","        fea_one = '/content/drive/MyDrive/fulingya/MDriver/%s/%s.fea' % (tumors_set[cancer_type], mode)\n","        df_one = pd.read_csv(fea_one, header=0, index_col=0, sep='\\t')\n","        #训练数据X\n","        mat_train_pos = df_one.loc[train_pos, ::].values.astype(float)\n","        mat_train_neg = df_one.loc[train_neg, ::].values.astype(float)\n","        X_train.append(np.concatenate([mat_train_pos, mat_train_neg]))\n","        #测试数据X\n","        mat_test = df_one.loc[test_ids, ::].values.astype(float)\n","        X_test.append(mat_test)\n","      if mode == 'mut2':\n","        fea_one = '/content/drive/MyDrive/fulingya/MDriver/%s/%s.fea' % (tumors_set[cancer_type], mode)\n","        df_one = pd.read_csv(fea_one, header=0, index_col=0, sep='\\t')\n","        fea_sublist=['AAA_ref','AAC_ref','AAG_ref','AAT_ref','ACA_ref','ACC_ref','ACG_ref','ACT_ref','AGA_ref','AGC_ref',\n","                      'AGG_ref','ATA_ref','ATC_ref','ATG_ref','CAA_ref','CAC_ref','CAG_ref','CCA_ref','CCC_ref',\n","                      'CCG_ref','CGA_ref','CGC_ref','CTA_ref','CTC_ref','GAA_ref','GAC_ref','GCA_ref','GCC_ref',\n","                      'GGA_ref','GTA_ref','TAA_ref','TCA_ref']\n","        mat_train_pos = df_one.loc[train_pos, fea_sublist].values.astype(float)\n","        mat_train_neg = df_one.loc[train_neg, fea_sublist].values.astype(float)\n","        X_train.append(np.concatenate([mat_train_pos, mat_train_neg]))\n","        #测试数据X\n","        mat_test = df_one.loc[test_ids, fea_sublist].values.astype(float)\n","        X_test.append(mat_test)\n","\n","    X_train=np.concatenate([X_train[0],X_train[1],X_train[2],X_train[3]],axis=1)\n","    Y_train = np.concatenate([np.ones((len(train_pos))), np.zeros((len(train_neg)))])\n","\n","    X_test=np.concatenate([X_test[0],X_test[1],X_test[2],X_test[3]],axis=1)\n","    cla_X_train=pd.DataFrame(X_train)\n","    cla_X_train['class']=Y_train\n","    geneid=np.concatenate([train_pos,train_neg])\n","    cla_X_train['geneid']=geneid\n","    return X_train, Y_train, X_test, cla_X_train\n","\n","lr=0.007\n","n_input=82\n","cuda = torch.cuda.is_available()\n","print(\"use cuda: {}\".format(cuda))\n","device = torch.device(\"cuda\" if cuda else \"cpu\")\n","class focal_loss(nn.Module):\n","    def __init__(self, alpha=0.25, gamma=2.0, num_classes=2, size_average=True):\n","        super(focal_loss, self).__init__()\n","        self.size_average = size_average\n","        if isinstance(alpha, list):\n","            self.alpha = torch.Tensor(alpha)\n","        else:\n","            self.alpha = torch.zeros(num_classes) #1行2列元素为0的张量\n","            self.alpha[0] += alpha\n","            self.alpha[1:] += (1-alpha)\n","        self.gamma = gamma\n","\n","    def forward(self, preds, labels):\n","        preds = preds.view(-1, preds.size(-1))\n","        self.alpha = self.alpha.to(preds.device)\n","        preds_logsoft = F.log_softmax(preds, dim=1)  # log_softmax\n","        preds_softmax = torch.exp(preds_logsoft)    # softmax\n","        preds_softmax = preds_softmax.gather(1, labels.view(-1, 1))\n","        preds_logsoft = preds_logsoft.gather(1, labels.view(-1, 1))\n","        self.alpha = self.alpha.gather(0, labels.view(-1))\n","        loss = -torch.mul(torch.pow((1-preds_softmax), self.gamma), preds_logsoft)\n","        loss = torch.mul(self.alpha, loss.t())\n","        if self.size_average:\n","            loss = loss.mean()\n","        else:\n","            loss = loss.sum()\n","        return loss\n","\n","get_custom_objects().update({'focal_loss': focal_loss})\n","\n","class MultiHeadSelfAttention(nn.Module):\n","    dim_in: int  # input dimension\n","    dim_k: int  # key and query dimension\n","    dim_v: int  # value dimension\n","    num_heads: int  # number of heads, for each head, dim_* = dim_* // num_heads\n","    def __init__(self, dim_in, dim_k, dim_v, num_heads=2):\n","        super(MultiHeadSelfAttention, self).__init__()\n","        assert dim_k % num_heads == 0 and dim_v % num_heads == 0, \"dim_k and dim_v must be multiple of num_heads\"\n","        self.dim_in = dim_in\n","        self.dim_k = dim_k\n","        self.dim_v = dim_v\n","        self.num_heads = num_heads\n","        self.linear_q = nn.Linear(dim_in, dim_k, bias=False)\n","        self.linear_k = nn.Linear(dim_in, dim_k, bias=False)\n","        self.linear_v = nn.Linear(dim_in, dim_v, bias=False)\n","        self._norm_fact = 1 / sqrt(dim_k // num_heads)\n","\n","    def forward(self, x):\n","        batch, dim_in = x.shape\n","        assert dim_in == self.dim_in\n","\n","        nh = self.num_heads  # 2\n","        dk = self.dim_k // nh  # dim_k of each head 1\n","        dv = self.dim_v // nh  # dim_v of each head 1\n","\n","        q = self.linear_q(x.reshape(batch, dim_in)).reshape(batch, nh, dk)  # (batch, nh, n, dk) 5.reshape(16,5,2)\n","        k = self.linear_k(x.reshape(batch, dim_in)).reshape(batch, nh, dk)  # (batch, nh, n, dk)\n","        v = self.linear_v(x.reshape(batch, dim_in)).reshape(batch, nh, dv)  # (batch, nh, n, dv)\n","\n","        dist = torch.matmul(q, k.transpose(1, 2)) * self._norm_fact  # batch, nh, n, n\n","        dist = torch.softmax(dist, dim=-1)  # batch, nh, n, n\n","\n","        att = torch.matmul(dist, v)  # batch, nh, n, dv\n","        att = att.transpose(1, 2).reshape(batch, self.dim_v)  # batch, n, dim_v\n","        return att\n","\n","class Transformer(nn.Module):\n","    def __init__(self, n_input,):\n","        super(Transformer, self).__init__()\n","        self.n = 59\n","        self.fc1 = nn.Linear(n_input, self.n)\n","        self.attn1 = MultiHeadSelfAttention(dim_in=self.n, dim_k=4, dim_v=18)\n","        self.fc2 = nn.Linear(18, 2)\n","        self.drop1 = nn.Dropout(0.0009)\n","\n","    def encoder(self, x):\n","        x1 = self.fc1(x)\n","        x1 = F.gelu(x1)\n","        x1 = self.drop1(x1)\n","        x2 = self.attn1(x1)\n","        x2 = F.gelu(x2)\n","        x5 = self.fc2(x2)\n","        x5 = F.softmax(x5)\n","        return x5\n","\n","    def forward(self, x):\n","        z = self.encoder(x)\n","        return z\n","\n","criteria = focal_loss(alpha=0.25, gamma=4)\n","\n","#加载数据\n","def load_data(path):\n","    df = pd.read_csv(path, sep=',')\n","    list = df.columns.values.tolist()\n","    list.remove('geneid')\n","    list.remove('label')\n","    #ss = StandardScaler()\n","    ss=MinMaxScaler()\n","    #ss=RobustScaler()\n","    #ss=MaxAbsScaler()\n","    df[list] = ss.fit_transform(df[list])\n","    joblib.dump(ss, '/content/drive/MyDrive/fulingya/Target/model/trans_predruggable.scaler')\n","    hg = df['geneid']\n","    targets = df['label']\n","    features = df.drop(['geneid', 'label'], axis=1)\n","    x = features.values\n","    y = targets.values\n","    y = y.astype(float)\n","    y = np.reshape(y, [len(y), 1])\n","    torch_data = MyDataset(x, y)\n","    return torch_data,hg\n","\n","class MyDataset(torch.utils.data.Dataset):\n","    def __init__(self, data_root, data_label):\n","        self.data = data_root\n","        self.label = data_label\n","\n","    def __getitem__(self, index):\n","        data = self.data[index]\n","        labels = self.label[index]\n","        return data, labels\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","#读取交叉验证的csv文件\n","def cv_data(batch_size):\n","  train_path = r'/content/drive/MyDrive/fulingya/Target/data/trans_pre_tra.csv'\n","  dataset, _ = load_data(train_path)\n","  train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=True) #这里shuffle不能设置为True\n","  for batch_idx, (x, y) in enumerate(train_loader):\n","    x=np.array(x)\n","    y=np.array(y)\n","  return x,y\n","\n","def train(batch_size1,batch_size2):\n","    train_path = r'/content/drive/MyDrive/fulingya/Target/data/trans_pre_tra.csv'\n","    dataset, _ = load_data(train_path)\n","    train_loader = DataLoader(dataset, batch_size=batch_size1, shuffle=False, drop_last=True) #这里shuffle不能设置为True\n","    for batch_idx, (x, y) in enumerate(train_loader):\n","      X_train=np.array(x)\n","      y_train=np.array(y)\n","    test_path = r'/content/drive/MyDrive/fulingya/Target/data/trans_pre_val.csv'\n","    dataset, _ = load_data(test_path)\n","    train_loader = DataLoader(dataset, batch_size=batch_size2, shuffle=False, drop_last=True) #这里shuffle不能设置为True\n","    for batch_idx, (x, y) in enumerate(train_loader):\n","      X_test=np.array(x)\n","      y_test=np.array(y)\n","    # print('X_train>>',X_train.shape)\n","    # print('y_train>>',y_train.shape)\n","    # print('X_test>>',X_test.shape)\n","    # print('y_test>>',y_test.shape)\n","\n","    model = Transformer(n_input=n_input, )\n","    optimizer = Adam(model.parameters(), lr=lr)\n","    X_test = torch.tensor(X_test)\n","    X_test = X_test.to(torch.float32)\n","\n","    epoch1 = 22 #24\n","    epoch2 = 122 #143-1900,139-2300\n","    # 半监督模块\n","    ratio = 0.1  # 缺失值比例 0.1,0.35\n","    # 产生一个随机状态种子\n","    rng = np.random.RandomState(10)  #10\n","    YSemi_train = np.copy(y_train)\n","    YSemi_train = YSemi_train.squeeze()\n","    # print('YSemi_train>>>', YSemi_train.shape)\n","    # rng.rand()返回一个或一组服从“0~1”均匀分布的随机样本值\n","    YSemi_train[rng.rand(len(y_train)) < ratio] = -1\n","    # 把每次验证的训练数据X_train中的部分数据变为无标签的\n","    unlabeledX = X_train[YSemi_train == -1,:]\n","    unlabeledY = y_train[YSemi_train == -1]\n","    unlabeledY = unlabeledY.squeeze()\n","    idx = np.where(YSemi_train != -1)[0]\n","    labeledX = X_train[idx, :]\n","    labeledY = YSemi_train[idx]\n","    probThreshold = 0.05 #0.05\n","    # 外层循环是半监督的次数 不需要损失\n","    for epoch in range(epoch1):\n","        probThreshold += 0.01 #0.01\n","        loss_list=[]\n","        # 内层循环是transformer的学习\n","        for i in range(epoch2):\n","          total_loss = 0.\n","          optimizer.zero_grad()\n","          labeledX = torch.tensor(labeledX).to(torch.float32)\n","          labeledY = torch.tensor(labeledY).to(torch.int64)\n","          # print(labeledX.shape)\n","          unlabeledX = torch.tensor(unlabeledX).to(torch.float32)\n","          unlabeledY = torch.tensor(unlabeledY).to(torch.int64)\n","          preds_train = model(labeledX)\n","          loss = criteria(preds_train, labeledY)\n","          loss_list.append(loss.detach().numpy())\n","          total_loss += loss\n","          loss.backward()\n","          optimizer.step()\n","        preds_unlabeledY = model(unlabeledX)\n","        preds_unlabeledY_Prob = preds_unlabeledY[:, 1]\n","        preds_unlabeledY_Prob = preds_unlabeledY_Prob.unsqueeze(1)\n","        preds_unlabeledY_Prob = preds_unlabeledY_Prob.detach().numpy()\n","        labelidx = np.where(preds_unlabeledY_Prob > probThreshold)[0]\n","        #print(labelidx.shape)\n","        unlabelidx = np.where(preds_unlabeledY_Prob <= probThreshold)[0]\n","        labeledX = np.array(labeledX)\n","        labeledY = np.array(labeledY)\n","        unlabeledX = np.array(unlabeledX)\n","        unlabeledY = np.array(unlabeledY)\n","        labeledX = np.vstack((labeledX, unlabeledX[labelidx, :]))\n","        labeledY = np.hstack((labeledY, unlabeledY[labelidx]))\n","        unlabeledX = unlabeledX[unlabelidx, :]\n","        unlabeledY = unlabeledY[unlabelidx]\n","\n","    preds_test = model(X_test)\n","    preds_te = preds_test[:, 1]\n","    preds_te = preds_te.unsqueeze(1)\n","    preds_te=preds_te.detach().numpy()\n","    fpr, tpr, thresholds = roc_curve(y_test, preds_te)\n","    roc_auc_1 = auc(fpr, tpr)\n","    print('AUROC：{:.4f}'.format(roc_auc_1))\n","    pr1=average_precision_score(y_test, preds_te)\n","    print('AUPRC：{:.4f}'.format(pr1))\n","    optimal_th, optimal_point = Find_Optimal_Cutoff(TPR=tpr, FPR=fpr, threshold=thresholds)\n","    precision, recall, thresholds = precision_recall_curve(y_test, preds_te)\n","    thresholds = np.append(thresholds, 1)\n","    # 寻找精确度不低于0.7的阈值\n","    optimal_threshold = thresholds[precision >= 0.75][0]\n","    print('精确度和召回率指标的最优门限：{}'.format(optimal_threshold))\n","    m = optimal_threshold\n","    joblib.dump(model, '/content/drive/MyDrive/fulingya/Target/model/WGFormer_predict.pkl')\n","    del model\n","    x1_all = []\n","    x2_all = []\n","    for i in preds_te[y_test == 0]:\n","        x1_all.append(i)\n","    for i in preds_te[y_test == 1]:\n","        x2_all.append(i)\n","    tatistic, pvalue = stats.mannwhitneyu(x1_all, x2_all, use_continuity=True, alternative='two-sided')  # 秩和检验\n","    print('roc_pvalue: {}'.format(pvalue))\n","    return m\n","\n","#预测非编码区可用药基因\n","def predict(X_test,m,test_ids,X_train,train_pos):\n","  df_fj = pd.read_csv(r'/content/drive/MyDrive/fulingya/Target/data/fuji.csv',sep=',')\n","  true_num = df_fj.iloc[:,0].values.tolist()\n","  X_test=torch.tensor(X_test)\n","  X_test = X_test.to(torch.float32)\n","  model=joblib.load('/content/drive/MyDrive/fulingya/Target/model/WGFormer_predict.pkl')\n","  probas_ = model(X_test)[:, 1]\n","  probas_numpy=probas_.detach().numpy()\n","\n","  probas_df = pd.DataFrame(probas_numpy)\n","  probas_df.columns = ['score']\n","\n","  drugable=0\n","  index=[]\n","  probas_list=list(probas_numpy)\n","  for i in range(len(probas_list)):\n","    if probas_list[i]>m:\n","      drugable+=1\n","      index.append(i)\n","  drugable_id=[]\n","  enhancers_id=0\n","  cds_id=0\n","  utr3_id=0\n","  utr5_id=0\n","  gcprom_id=0\n","  lncrna_prom_id=0\n","  lncrna_id=0\n","  lncrna_ncrna_id=0\n","  lncrna_ss_id = 0\n","  mirna_id=0\n","  cds_id_pos=0\n","  ch_ids=0\n","  ch_grade = 0\n","  smallrna_id = 0\n","  noncds_id_pos=0\n","  other_id=0\n","  drugable_score=[]\n","  ch_grade_list=[]\n","  fuji_druggable = []\n","\n","  for i in index:\n","    drugable_id.append(test_ids[i])\n","    drugable_score.append(probas_list[i])\n","    tmps = re.split('::', test_ids[i])\n","    gene = tmps[2]\n","    reg = tmps[0]\n","    if 'enhancers' in reg:\n","      enhancers_id+=1\n","    if 'gc19_pc.cds' in reg:\n","      cds_id+=1\n","    if 'gc19_pc.3utr' in reg:\n","      utr3_id+=1\n","    if 'gc19_pc.5utr' in reg:\n","      utr5_id+=1\n","    if 'gc19_pc.promCore' in reg:\n","      gcprom_id+=1\n","    if 'lncrna.promCore' in reg:\n","      lncrna_prom_id+=1\n","      fuji_druggable.append(test_ids[i])\n","    if 'lncrna.ncrna' in reg:\n","      lncrna_ncrna_id+=1\n","      fuji_druggable.append(test_ids[i])\n","    if 'lncrna.ss' in reg:\n","      lncrna_ss_id+=1\n","      fuji_druggable.append(test_ids[i])\n","    if 'mirna' in reg:\n","      mirna_id+=1\n","      fuji_druggable.append(test_ids[i])\n","    if 'smallrna' in reg:\n","      smallrna_id+=1\n","    if 'enhancers' not in reg and 'gc19_pc.cds' not in reg and 'gc19_pc.3utr' not in reg and 'gc19_pc.5utr' not in reg and 'gc19_pc.promCore' not in reg and 'lncrna.promCore' not in reg and 'lncrna.ncrna' not in reg:\n","      other_id+=1\n","\n","  drugable_id_pd=pd.DataFrame(drugable_id)\n","  drugable_id_pd.columns = ['geneid']\n","  drugable_score_pd=pd.DataFrame(drugable_score)\n","  drugable_score_pd.columns = ['score']\n","  drugable_index_pd=pd.DataFrame(index)\n","  drugable_index_pd.columns = ['index_i']\n","  drugable_id_score=pd.concat([drugable_id_pd,drugable_score_pd,drugable_index_pd],axis=1)\n","  drugable_id_score.sort_values(by=['geneid','score'], ascending=True, inplace=True)\n","  drugable_id_score.to_csv(r'/content/drive/MyDrive/fulingya/Target/result/targetGenes_credible.csv', index=False)\n","  print(drugable)\n","  print('enhancers({})'.format(enhancers_id))\n","  print('gc19_pc.cds({})'.format(cds_id))\n","  print('gc19_pc.utr3({})'.format(utr3_id))\n","  print('gc19_pc.utr5({})'.format(utr5_id))\n","  print('gc19_pc.prom({})'.format(gcprom_id))\n","  print('mirna({})'.format(mirna_id))\n","  print('smallrna({})'.format(smallrna_id))\n","  print('lncrna.prom({})'.format(lncrna_prom_id))\n","  print('lncrna.ncrna({})'.format(lncrna_ncrna_id))\n","  print('lncrna.ss({})'.format(lncrna_ss_id))\n","\n","  # 计算预测可成药基因与另找的noncorna数据集的交集，ncRNADrug.csv\n","  pd_rna_drivers = pd.read_csv('/content/drive/MyDrive/fulingya/Target/data/ncRNADrug.csv')\n","  noncds_rna_list = pd_rna_drivers['geneid'].values.tolist()\n","  noncds_rna_ids = list(set(noncds_rna_list))\n","  jiaoji1 = 0\n","  for i in drugable_id:\n","    if i in noncds_rna_ids:\n","      jiaoji1 += 1\n","  print('预测可成药基因与另找的noncorna数据集交集的数量',jiaoji1)\n","\n","\n","\n","  #富集分析,与另找的noncorna数据集计算富集\n","  true_num = noncds_rna_ids\n","  #在集子里同时又被预测为可成药,fuji_druggable指有多少mirna和lncrna被预测为可成药\n","  num_above_threshold_true = len(drugable_id_pd[drugable_id_pd['geneid'].isin(true_num)].index)\n","  print('在集子里同时又被预测为可成药的数量为:{}'.format(num_above_threshold_true))\n","  #不在集子里同时又被预测为可成药\n","  #num_above_threshold_false = len(drugable_id) - num_above_threshold_true\n","  num_above_threshold_false = len(fuji_druggable) - num_above_threshold_true\n","  print('不在集子里同时又被预测为可成药的数量为:{}'.format(num_above_threshold_false))\n","  #在集子里但是没有被预测为可成药\n","  #14万基因中一共有多少个mirna和lncrna 21025个,去掉tju和smallrna\n","  rna_ids = []\n","  for id in test_ids:\n","    tmps = re.split('::', id)\n","    reg = tmps[0]\n","    if 'mirna' in reg or 'lncrna' in reg:\n","      rna_ids.append(id)\n","  #print('rna_ids>>>>>>>>',len(rna_ids))\n","  df_rna_ids = pd.DataFrame(rna_ids)\n","  df_rna_ids.columns = ['geneid']\n","  df_false = df_rna_ids[~df_rna_ids['geneid'].isin(fuji_druggable)]\n","  num_below_threshold_true = len(df_false[df_false['geneid'].isin(true_num)].index)\n","  print('在集子里但是没有被预测为可成药的数量为:{}'.format(num_below_threshold_true))\n","  #不在集子里也没有被预测为可成药\n","  num_below_threshold_false = len(rna_ids) - num_above_threshold_true - num_above_threshold_false - num_below_threshold_true\n","  print('不在集子里也没有被预测为可成药的数量为:{}'.format(num_below_threshold_false))\n","  #精确检验返回值\n","  a = num_above_threshold_true\n","  b = num_above_threshold_false\n","  c = num_below_threshold_true\n","  d = num_below_threshold_false\n","  p = fisher_ex(a, b, c, d)\n","  print('Pancan Tier2富集分析p：{}'.format(p))\n","\n","def main(argv=sys.argv):\n","    parser = argparse.ArgumentParser(description='cds_pre_noncds')\n","    parser.add_argument(\"-m\", dest='mode', default=\"pred\", help=\"mode\")\n","    parser.add_argument(\"-t\", dest='type', default=\"Pancan\", help=\"cancer type\")\n","    parser.add_argument(\"-o\", dest='out', default=\"/content/drive/MyDrive/fulingya/\", help=\"coding file\")\n","    args = parser.parse_args(args=[])\n","    cancer_type=args.type\n","\n","    df_tmp = pd.read_csv(r'/content/drive/MyDrive/fulingya/MDriver/chr_id.txt', header=0, index_col=3, sep='\\t', usecols=[0, 1, 2, 3])\n","    all_list = df_tmp.index.tolist()\n","    #print(len(all_list)) 146586\n","\n","    train_pos, train_neg, test_ids = build_set(all_list)\n","    X_train, Y_train, X_test, cla_X_train = file2data(args.type, train_pos, train_neg, test_ids)\n","    print(X_test.shape)\n","    #将交叉验证部分的训练数据保存为csv文件\n","    train_geneid = np.concatenate([train_pos, train_neg])\n","    df_train_geneid = pd.DataFrame(train_geneid)\n","    df_train_geneid.columns=['geneid']\n","    df_X_train = pd.DataFrame(X_train)\n","    df_Y_train = pd.DataFrame(Y_train)\n","    df_Y_train.columns = ['label']\n","    feature_train = pd.concat([df_train_geneid, df_X_train, df_Y_train], axis=1)\n","    feature_train.columns=['geneid','freq_Intron','freq_IGR','freq_RNA','freq_Missense_Mutation','freq_3UTR','freq_lincRNA',\n","              'freq_5Flank','freq_Silent','freq_5UTR','freq_Splice_Site','freq_Nonsense_Mutation',\n","              'freq_De_novo_Start_OutOfFrame','freq_Frame_Shift_Del','freq_In_Frame_Del',\n","              'freq_Frame_Shift_Ins','freq_De_novo_Start_InFrame','freq_Start_Codon_SNP',\n","              'freq_In_Frame_Ins','freq_Nonstop_Mutation','freq_Start_Codon_Del','freq_Stop_Codon_Del',\n","              'freq_Stop_Codon_Ins','freq_Start_Codon_Ins','freq_SNP','freq_DNP','freq_TNP','freq_DEL',\n","              'freq_INS','freq_ONP','sample_count_mean','sample_count_var','gc_mean','gc_var',\n","              'amp_mean','amp_var','amp_freq','del_mean','del_var','del_freq','abs_mean','abs_var','abs_freq',\n","              'exp_mean','exp_var','rep_time','exp_CCLE','AAA_ref','AAC_ref','AAG_ref','AAT_ref','ACA_ref','ACC_ref','ACG_ref','ACT_ref','AGA_ref','AGC_ref',\n","              'AGG_ref','ATA_ref','ATC_ref','ATG_ref','CAA_ref','CAC_ref','CAG_ref','CCA_ref','CCC_ref',\n","              'CCG_ref','CGA_ref','CGC_ref','CTA_ref','CTC_ref','GAA_ref','GAC_ref','GCA_ref','GCC_ref',\n","              'GGA_ref','GTA_ref','TAA_ref','TCA_ref','CADD_mean','CADD_var','CSS_mean','CSS_var','label']\n","    #feature_train.to_csv(r'/content/drive/MyDrive/fulingya/Target/data/trans_pre_CV.csv', index=False)\n","    print(feature_train.shape[0]) #4818\n","    length = int(feature_train.shape[0] * 0.9) #0.8\n","    print('length>>>',length)\n","    feature_train=feature_train.sample(frac=1.0,random_state=1)#0-3\n","    feature_train_tra=feature_train.iloc[:length]\n","    print(feature_train_tra.shape)\n","    feature_train_tra.to_csv(r'/content/drive/MyDrive/fulingya/Target/data/trans_pre_tra.csv', index=False)\n","    feature_train_va=feature_train.iloc[length:]\n","    print(feature_train_va.shape)\n","    feature_train_va.to_csv(r'/content/drive/MyDrive/fulingya/Target/data/trans_pre_val.csv', index=False)\n","    m = train(length,feature_train.shape[0]-length)\n","    ss=joblib.load('/content/drive/MyDrive/fulingya/Target/model/trans_predruggable.scaler')\n","    X_train=ss.fit_transform(X_train)\n","    X_test=ss.transform(X_test)\n","    predict(X_test,m,test_ids,X_train,train_pos)\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","source":[],"metadata":{"id":"ohHGFf-yih94"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kaybGtajiiAH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"B1RDJAs1iiC7"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}